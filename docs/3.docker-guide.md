---
title: Running Stable Diffusion WebUI Using Docker
---
<!--
This file is part of stable-diffusion-webui (https://github.com/sd-webui/stable-diffusion-webui/).

Copyright 2022 sd-webui team.
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
-->

## Running prebuilt image

The easiest way to run Stable Diffusion WebUI is to use the prebuilt image from Docker Hub.

```bash
docker pull hlky/sd-webui:runpod
```
This image has all the necessary models baked in. It is quite large but streamlines the process of managing the various models and simplifies the user experience.

Alternatively you can pull:
```bash
docker pull hlky/sd-webui:latest
```
This image includes the babrebones environment to run the Web UI. The models will be downloaded during the installation process. You will have to take care of the volume for the `sd/models` directory.


It is recommended that you run the `runpod` version.
You can run the image using the following command:
```bash
docker container run --rm -d -p 8501:8501 -e STREAMLIT_SERVER_HEADLESS=true -e "WEBUI_SCRIPT=webui_streamlit.py" -e "VALIDATE_MODELS=false" -v "${PWD}/outputs:/sd/outputs" --gpus all hlky/sd-webui:runpod
```

> Note: if you are running it on runpod it only supports one volume mount which is used for your outputs.

> Note: if you are running it on your local machine the output directory will be created in the current directory from where you run this command.

## Building the image

This Docker environment is intended to speed up development and testing of Stable Diffusion WebUI features. Use of a container image format allows for packaging and isolation of Stable Diffusion / WebUI's dependencies separate from the Host environment.

You can use this Dockerfile to build a Docker image and run Stable Diffusion WebUI locally.


Requirements:
* Host computer is AMD64 architecture (e.g. Intel/AMD x86 64-bit CPUs)
* Host computer operating system (Linux or Windows with WSL2 enabled)
    * See [Microsoft WSL2 Installation Guide for Windows 10] (https://learn.microsoft.com/en-us/windows/wsl/) for more information on installing.
    * Ubuntu (Default) for WSL2 is recommended for Windows users 
* Host computer has Docker, or compatible container runtime
    * Docker Compose (v1.29+) or later 
    * See [Install Docker Engine] (https://docs.docker.com/engine/install/#supported-platforms) to learn more about installing Docker on your Linux operating system
* 10+ GB Free Disk Space (used by Docker base image, the Stable Diffusion WebUI Docker image for dependencies, model files/weights)

Additional Requirements:
* Host computer is equipped with a CUDA-compatible GPU (e.g. Nvidia RTX 2xxx, 3000x)
* NVIDIA Container Toolkit is installed
    * See [NVIDIA Container Toolkit Installation Guide] (https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#supported-platforms "Official NVIDIA Installation Guide") for more information on installing.

Other Notes:
* "Optional" packages commonly used with Stable Diffusion WebUI workflows such as, RealESRGAN, GFPGAN, will be installed by default.
* An older version of running Stable Diffusion WebUI using Docker exists here: https://github.com/sd-webui/stable-diffusion-webui/discussions/922

### But what about AMD?
There is tentative support for AMD GPUs through docker which can be enabled via `docker-compose.amd.yml`,
although this is still in the early stages. Right now, this _only_ works on native linux (not WSL2) due
to issues with AMDs support of GPU passthrough. You also _must_ have ROCm drivers installed on the host.
```
    docker compose -f docker-compose.yml -f docker-compose.amd.yml ...
```
or, by setting 
```
    export COMPOSE_FILE=docker-compose.yml:docker-compose.amd.yml
```
in your `.profile` or through a tool like `direnv`


---

## First-Time Startup Instructions

### Clone Repository
* Clone this repository to your host machine:
    * `git clone https://github.com/sd-webui/stable-diffusion-webui.git`
* If you plan to use Docker Compose to run the image in a container (most users), create an `.env_docker` file using the example file:
    * `cp .env_docker.example .env_docker`
    * Edit `.env_docker` using the text editor of your choice.
    * Ensure `VALIDATE_MODELS` is set to `true`
> Options available in `.env_docker` allow you to control automatic model file checking/download during startup, and to select the Stable Diffusion WebUI implementation to run (Gradio vs Streamlit). You may the set `VALIDATE_MODELS` option to `false` after the initial run (once models are downloaded) to speed up startup time.


### Create a Container Instance Using Docker Compose
The default `docker-compose.yml` file  will create a Docker container instance named `sd-webui`

* Create an instance of the Stable Diffusion WebUI image as a Docker container:
    * `docker compose up`
* During the first run, the container image will be build containing all of the dependencies necessary to run Stable Diffusion. This build process will take several minutes to complete
    * After the image build has completed, you will have a docker image for running the Stable Diffusion WebUI tagged `stable-diffusion-webui:dev`

(Optional) Daemon mode:
* You can start the container in "daemon" mode by applying the `-d` option:  `docker compose up -d`. This will run the server in the background so you can close your console window without losing your work.
* When running in daemon mode, you can view logging output from your container by running `docker logs sd-webui`

> Note: Depending on your version of Docker/Docker Compose installed, the command may be `docker-compose` (older versions) or `docker compose` (newer versions)


### Accessing your Stable Diffusion WebUI Instance
The container may take several minutes to start up if model weights/checkpoints need to be downloaded. You can view progress via `docker compose ps` to see the current status or by checking the logs using `docker compose logs`.

Depending on the WebUI implementation you selected in `.env_docker`, you can access the WebUI at the following URLs:

* Gradio:  http://localhost:7860
* Streamlit:  http://localhost:8501

You can expose and access your WebUI to/from remote hosts by the machine's IP address:
(note: This generally does not apply to Windows/WSL2 users due to WSL's implementation)
* Gradio:  http://\<host-ip-address\>:7860
* Streamlit:  http://\<host-ip-address\>:8501


### Where is ___ stored?

By default, model weights/checkpoint files will be stored at the following path:
* `./model_cache/`

Output files generated by Stable Diffusion will be stored at the following path:
* `./output/`

The above paths will be accessible directly from your Docker container's host.


### Shutting down your Docker container
You can stop your Docker container by pressing the `CTRL+C` key combination in the terminal where the container was started..

If you started the container using `docker compose`, you can stop the container with the command:
* `docker compose down`

Using the default configuration, your Stable Diffusion output, cached model weights/files, etc will persist between Docker container starts.

---

## Resetting your Docker environment
Should you need to do so, the included `docker-reset.sh` script will remove all docker images, stopped containers, and cached model weights/checkpoints.

You will need to re-download all associated model files/weights used by Stable Diffusion WebUI, which total to several gigabytes of data. This will occur automatically upon the next startup.


## Misc Related How-to
* You can obtain shell access to a running Stable Diffusion WebUI container started with Docker Compose with either of the following commands:
    * `docker exec -it st-webui /bin/bash`
    * `docker compose exec stable-diffusion bash`
* To start a container using the Stable Diffusion WebUI Docker image without Docker Compose, you can do so with the following command:
    * `docker run --rm -it --entrypoint /bin/bash stable-diffusion-webui:dev`
* To start a container, with mapped ports, GPU resource access, and a local directory bound as a container volume, you can do so with the following command:
    * `docker run --rm -it -p 8501:8501 -p 7860:7860 --gpus all -v $(pwd):/sd --entrypoint /bin/bash stable-diffusion-webui:dev`

---

## Dockerfile Implementation Notes
Compared to base Stable Diffusion distribution, Conda-based package management was removed.

The Pytorch base image with Nvidia CUDA support is used as the base Docker image to simplify dependencies.

Python dependency requirements for various packages used by Stable Diffusion WebUI have been separated into different groups. During the container image build process, requirements are installed in the following order:

1. Stable Diffusion (core) requirements (`sd_requirements.txt`)
2. General Requirements (`requirements.txt`)
3. External optional packages requirements (`ext_requirements.txt`)
4. WebUI requirements (`ui_requirements.txt`)

Python package dependencies have been version-pinned where possible.

**Developers:  When developing new features or making changes to the environment that require dependency changes, please update and make notes in the appropriate file to help us better track and manage dependencies.**

### Other Notes

* The `root_profile` Docker Volume
    * The `huggingface/transformers` package will download files to a cache located at `/root/.cache/huggingface/transformers` totalling nearly ~1.6 GB
